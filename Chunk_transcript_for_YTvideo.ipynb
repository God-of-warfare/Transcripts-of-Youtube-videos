{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6hUyl-E7WTO"
      },
      "outputs": [],
      "source": [
        "!pip install yt-dlp transformers huggingface_hub librosa soundfile noisereduce pydub deepmultilingualpunctuation sentence_transformers scikit-learn\n",
        "\n",
        "import yt_dlp\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "import soundfile as sf\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import noisereduce as nr\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "import librosa\n",
        "import json\n",
        "import torch\n",
        "import string\n",
        "from typing import Dict, List\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "import re\n",
        "import gc\n",
        "import time\n",
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "from pydub import AudioSegment\n",
        "\n",
        "#### UTILS ###########\n",
        "def split_at_center_comma(input_string):\n",
        "    \"\"\"\n",
        "    Takes input a sentence and splits it into 2 near the center-most comma\n",
        "    \"\"\"\n",
        "    comma_indices = [i for i, char in enumerate(input_string) if char == ',']\n",
        "\n",
        "    if not comma_indices:\n",
        "        return input_string, \"\"\n",
        "\n",
        "    center_index = len(comma_indices) // 2\n",
        "    center_comma_index = comma_indices[center_index]\n",
        "\n",
        "    first_part = input_string[:center_comma_index]\n",
        "    second_part = input_string[center_comma_index + 1:]  # Skip the comma\n",
        "\n",
        "    return first_part, second_part\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    \"\"\"\n",
        "    Takes a string and returns a list of all sentences in it\n",
        "    \"\"\"\n",
        "    # Split the text at full stops,?,!\n",
        "    sentences = re.split(r'[.?!]', text)\n",
        "    # Remove any leading/trailing whitespace and filter out empty sentences\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    return sentences\n",
        "\n",
        "def format_data_chunks(data: List[Dict[str,List[float]]]):\n",
        "    for i in range(len(data)):\n",
        "        try:\n",
        "            data[i]['text'] = data[i]['text'].translate(str.maketrans('', '', string.punctuation))  # Remove punctuations\n",
        "\n",
        "            if data[i]['text'] == '000':\n",
        "\n",
        "                data[i-1]['text'] = data[i-1]['text'] + data[i]['text']\n",
        "\n",
        "                del data[i]\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def check_if_chunk_is_long_enough(chunk):\n",
        "        \"\"\"\n",
        "        Checks if a particular chunk is bigger than 15s\n",
        "        \"\"\"\n",
        "        if chunk['end_time'] - chunk['start_time'] > 15:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "def download_audio(url):\n",
        "    '''\n",
        "    Downloads the audio from YT\n",
        "    '''\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'outtmpl': 'audio_extracted.%(ext)s',\n",
        "    }\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([url])\n",
        "        print(\"Audio downloaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def clean_chunks(chunks):\n",
        "    \"\"\"\n",
        "    Removes filler words from the text of the chunks and merges chunks with less than 3 words while ensuring they don't exceed 15s\n",
        "    \"\"\"\n",
        "    filler_words = {'ah', 'uh', 'uhm','',' ah', 'ah,','ah ','uh ','uhm ','uhm,','uh,',' uh',' uhm',' uhm,',' uhm',' uh',' uh ',' uhm',' uhm ',' uhm'}\n",
        "    processed = []\n",
        "    i = 0\n",
        "    while i < len(chunks):\n",
        "        chunk = chunks[i]\n",
        "        # Split text into words and remove filler words\n",
        "        words = [word for word in chunk['text'].split() if word.lower() not in filler_words]\n",
        "        text = ' '.join(words)\n",
        "        # If no words left after removal, skip this chunk\n",
        "        if not text:\n",
        "            i += 1\n",
        "            continue\n",
        "        # Update the chunk's text\n",
        "        chunk['text'] = text\n",
        "        # Calculate duration\n",
        "        duration = chunk['end_time'] - chunk['start_time']\n",
        "        # Check if text has less than 3 words and duration < 15 seconds\n",
        "        if len(words) < 3 and duration < 15:\n",
        "            # Try to merge with the previous chunk if possible\n",
        "            if len(processed) > 0:\n",
        "                prev_chunk = processed[-1]\n",
        "                combined_duration = prev_chunk['end_time'] - prev_chunk['start_time'] + duration\n",
        "                if combined_duration < 15:\n",
        "                    # Merge with previous chunk\n",
        "                    prev_chunk['end_time'] = chunk['end_time']\n",
        "                    prev_chunk['text'] += ' ' + chunk['text']\n",
        "                    i += 1\n",
        "                    continue\n",
        "            # Try to merge with the next chunk if possible\n",
        "            if i + 1 < len(chunks):\n",
        "                next_chunk = chunks[i + 1]\n",
        "                combined_duration = next_chunk['end_time'] - chunk['start_time']\n",
        "                if combined_duration < 15:\n",
        "                    # Merge with next chunk\n",
        "                    chunk['end_time'] = next_chunk['end_time']\n",
        "                    chunk['text'] += ' ' + next_chunk['text']\n",
        "                    # Skip the next chunk\n",
        "                    i += 2\n",
        "                    processed.append(chunk)\n",
        "                    continue\n",
        "            # If merging is not possible, keep the chunk as is\n",
        "            processed.append(chunk)\n",
        "            i += 1\n",
        "            continue\n",
        "        # If chunk has 3 or more words or duration >= 15, keep it as is\n",
        "        processed.append(chunk)\n",
        "        i += 1\n",
        "    return processed\n",
        "\n",
        "def reduce_to_less_than_15(chunks: List, timestamps):\n",
        "    \"\"\"\n",
        "    Takes input a list of chunks and tris to reduce them to less than 15s by breaking the text at a comma and also adjusting the timestamps accordingly based on the word-level timestamps\n",
        "    \"\"\"\n",
        "    reduced_chunks = []\n",
        "\n",
        "    for i in range(len(chunks)):\n",
        "        begin_time = chunks[i]['start_time']\n",
        "        end_time = chunks[i]['end_time']\n",
        "\n",
        "        words = []\n",
        "        split_sentence1, split_sentence2 = split_at_center_comma(chunks[i]['text'])\n",
        "        end_time1 = 0\n",
        "        start_time2 = 0\n",
        "\n",
        "        # Gather all words that are within the time range\n",
        "        for i in range(len(timestamps)):\n",
        "            if timestamps[i]['timestamp'][0] >= begin_time and timestamps[i]['timestamp'][1] <= end_time:\n",
        "                words.append(timestamps[i])\n",
        "\n",
        "        last_wrd_sentence1 = split_sentence1.split()[-1]\n",
        "\n",
        "        for i in range(len(words)):\n",
        "            if words[i]['text'].strip().lower() == last_wrd_sentence1.strip().lower():\n",
        "                end_time1 = words[i]['timestamp'][0] + 0.01\n",
        "                start_time2 = words[i+1]['timestamp'][0] - 0.01\n",
        "                break\n",
        "\n",
        "\n",
        "        chunk1 = {\"text\": split_sentence1,\"start_time\": begin_time,\"end_time\": end_time1}\n",
        "        chunk2 = {\"text\": split_sentence2,\"start_time\": start_time2,\"end_time\": end_time}\n",
        "\n",
        "\n",
        "        reduced_chunks.append(chunk1)\n",
        "        reduced_chunks.append(chunk2)\n",
        "\n",
        "    return reduced_chunks\n",
        "\n",
        "\n",
        "def prepare_for_output(chunks):\n",
        "  for i in range(len(chunks)):\n",
        "    chunks[i]['chunk_id'] = i\n",
        "    chunks[i]['chunk_length'] = chunks[i]['end_time'] - chunks[i]['start_time']\n",
        "\n",
        "  return chunks\n",
        "\n",
        "\n",
        "##########UTILS END############\n",
        "\n",
        "def create_proper_chunks(list_of_sentences: List[str],timestamps: Dict[str, List[float]]):\n",
        "    chunks = []\n",
        "    word_level_timestamps = list(timestamps)  # List of dictionaries\n",
        "\n",
        "    def split_sentence(sentence: str):\n",
        "        words = sentence.replace(\"-\", \" \").split()\n",
        "        sentence_words = []\n",
        "        for word in words:\n",
        "            # 2. Remove punctuation\n",
        "            word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "            if word:  # Add only if the word is not empty after punctuation removal\n",
        "                sentence_words.append(word)\n",
        "\n",
        "        return sentence_words\n",
        "\n",
        "\n",
        "    # Create a flat list of all words from list_of_sentences\n",
        "    all_sentence_words = []\n",
        "    for sentence in list_of_sentences:\n",
        "        # 1. Split at hyphens first\n",
        "        words = sentence.replace(\"-\", \" \").split()\n",
        "\n",
        "        for word in words:\n",
        "            # 2. Remove punctuation\n",
        "            word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "            if word:  # Add only if the word is not empty after punctuation removal\n",
        "                all_sentence_words.append(word)\n",
        "\n",
        "    # Check if the number of words matches\n",
        "    if len(all_sentence_words) != len(word_level_timestamps):\n",
        "        raise ValueError(f\"The total number of words in sentences does not match the number of word timestamps {len(all_sentence_words)} != {len(word_level_timestamps)}\")\n",
        "\n",
        "    word_counter = 0\n",
        "    for i in range(len(list_of_sentences)):\n",
        "        sentence = list_of_sentences[i]\n",
        "        start_time = 0\n",
        "        end_time = 0\n",
        "        begin_word_counter = word_counter\n",
        "        end_word_counter = word_counter\n",
        "        words = split_sentence(sentence)\n",
        "\n",
        "        for j in range(len(words)):\n",
        "\n",
        "            word = words[j]\n",
        "            if word.lower().strip() == word_level_timestamps[word_counter]['text'].strip().lower():\n",
        "                if j == 0:\n",
        "                    start_time = word_level_timestamps[word_counter]['timestamp'][0] - 0.25\n",
        "                if j == len(words) - 1:\n",
        "                    end_time = word_level_timestamps[word_counter]['timestamp'][1] + 0.25\n",
        "                    end_word_counter = word_counter\n",
        "\n",
        "            if word.lower().strip() != word_level_timestamps[word_counter]['text'].lower().strip():\n",
        "                print(word)\n",
        "                print(word_level_timestamps[i]['text'])\n",
        "\n",
        "            word_counter += 1\n",
        "\n",
        "        if end_time - start_time > 15:\n",
        "            # split sentence in half at a comma\n",
        "            if ',' in sentence:\n",
        "                split_sentence1, split_sentence2 = split_at_center_comma(sentence)\n",
        "\n",
        "                words1 = split_sentence(split_sentence1)\n",
        "                words2 = split_sentence(split_sentence2)\n",
        "\n",
        "                start_time1 = end_time1 = 0\n",
        "                start_time2 = end_time2 = 0\n",
        "\n",
        "                for j in range(len(words1)):\n",
        "                    word = words1[j]\n",
        "                    if word.lower().strip() == word_level_timestamps[begin_word_counter]['text'].strip().lower():\n",
        "                        if j == 0:\n",
        "                            start_time1 = word_level_timestamps[begin_word_counter]['timestamp'][0] - 0.25\n",
        "                        if j == len(words1) - 1:\n",
        "                            end_time1 = word_level_timestamps[begin_word_counter]['timestamp'][1] + 0.25\n",
        "\n",
        "                    if word.lower().strip() != word_level_timestamps[begin_word_counter]['text'].lower().strip():\n",
        "                        print(word)\n",
        "                        print(word_level_timestamps[i]['text'])\n",
        "\n",
        "                    begin_word_counter += 1\n",
        "\n",
        "                for j in range(len(words2)):\n",
        "                    word = words2[j]\n",
        "                    if word.lower().strip() == word_level_timestamps[begin_word_counter]['text'].strip().lower():\n",
        "                        if j == 0:\n",
        "                            start_time2 = word_level_timestamps[begin_word_counter]['timestamp'][0] - 0.01\n",
        "                        if j == len(words2) - 1:\n",
        "                            end_time2 = word_level_timestamps[begin_word_counter]['timestamp'][1] + 0.01\n",
        "\n",
        "                    if word.lower().strip() != word_level_timestamps[begin_word_counter]['text'].lower().strip():\n",
        "                        print(word)\n",
        "                        print(word_level_timestamps[i]['text'])\n",
        "\n",
        "                    begin_word_counter += 1\n",
        "\n",
        "\n",
        "                dictionary1 = {'text': split_sentence1}\n",
        "                dictionary1['start_time'] = start_time1\n",
        "                dictionary1['end_time'] = end_time1\n",
        "\n",
        "                dictionary2 = {'text': split_sentence2}\n",
        "                dictionary2['start_time'] = start_time2\n",
        "                dictionary2['end_time'] = end_time2\n",
        "\n",
        "                chunks.append(dictionary1)\n",
        "                chunks.append(dictionary2)\n",
        "\n",
        "        else:\n",
        "            dictionary = {'text': sentence}\n",
        "            dictionary['start_time'] = start_time\n",
        "            dictionary['end_time'] = end_time\n",
        "            chunks.append(dictionary)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "#### Preprocessing audio\n",
        "def preprocess_audio(input_file, output_file='audio_processed.wav', target_sr=16000):\n",
        "    data, sr = sf.read(input_file)\n",
        "    # Convert to mono if stereo\n",
        "    if data.ndim > 1:\n",
        "        data = data.mean(axis=1)\n",
        "\n",
        "    # Resample if necessary\n",
        "    if sr != target_sr:\n",
        "        data = librosa.resample(data, orig_sr=sr, target_sr=target_sr)\n",
        "    # Noise reduction\n",
        "    reduced_noise = nr.reduce_noise(y=data, sr=target_sr)\n",
        "    # Save the processed audio\n",
        "    sf.write(output_file, reduced_noise, target_sr)\n",
        "\n",
        "\n",
        "#### Transcribing audio using Whisper\n",
        "def transcribe_audio_with_timestamps(audio_file, model_name='openai/whisper-large-v3-turbo',chunk_length=30):\n",
        "    \"\"\"\n",
        "    Transcribe an audio file in chunks and return transcription with timestamps.\n",
        "    \"\"\"\n",
        "\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_name, torch_dtype=torch_dtype, low_cpu_mem_usage=False, use_safetensors=True\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "    pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    chunk_length_s=chunk_length,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        "    )\n",
        "\n",
        "\n",
        "    result = pipe(audio_file,return_timestamps='word')\n",
        "    model.to('cpu')  # Move model to CPU\n",
        "    del model        # Delete the model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def punctuate(text: str):\n",
        "    \"\"\"\n",
        "    Punctuate the given text using a finetuned bert model.\n",
        "    \"\"\"\n",
        "    model = PunctuationModel()\n",
        "    return model.restore_punctuation(text)\n",
        "\n",
        "def merge_chunks(data, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\", similarity_threshold=0.2, max_duration=15):\n",
        "    \"\"\"\n",
        "    Merges semantically similar and continuous chunks of text based on embeddings\n",
        "    and duration constraints.\n",
        "\n",
        "    Args:\n",
        "        data (list of dict): A list of dictionaries, where each dictionary has\n",
        "                             'start_time', 'end_time', and 'text' keys.\n",
        "        embedding_model (str, optional): The name of the sentence transformer model\n",
        "                                         to use. Defaults to \"all-MiniLM-L6-v2\".\n",
        "        similarity_threshold (float, optional): The minimum cosine similarity score\n",
        "                                               for merging chunks. Defaults to 0.8.\n",
        "        max_duration (float, optional): The maximum allowed duration for a merged chunk\n",
        "                                       in seconds. Defaults to 15.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: A list of merged dictionaries with updated 'start_time', 'end_time',\n",
        "                      and 'text'.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Embeddings\n",
        "    model = SentenceTransformer(embedding_model)\n",
        "    texts = [d[\"text\"] for d in data]\n",
        "    embeddings = model.encode(texts)\n",
        "\n",
        "    merged_chunks = []\n",
        "    i = 0\n",
        "    while i < len(data):\n",
        "        current_chunk = data[i]\n",
        "        current_embedding = embeddings[i]\n",
        "        j = i + 1\n",
        "        while j < len(data):\n",
        "            # Check duration constraint\n",
        "            if data[j][\"end_time\"] - current_chunk[\"start_time\"] > max_duration:\n",
        "                break\n",
        "\n",
        "            # Check semantic similarity\n",
        "            similarity = cosine_similarity(\n",
        "                current_embedding.reshape(1, -1), embeddings[j].reshape(1, -1)\n",
        "            )[0][0]\n",
        "\n",
        "            if similarity >= similarity_threshold:\n",
        "                # Merge\n",
        "                current_chunk[\"end_time\"] = data[j][\"end_time\"]\n",
        "                current_chunk[\"text\"] += \" \" + data[j][\"text\"]\n",
        "                current_embedding = model.encode([current_chunk[\"text\"]])[\n",
        "                    0\n",
        "                ]  # Update embedding\n",
        "                j += 1\n",
        "            else:\n",
        "                break\n",
        "        merged_chunks.append(current_chunk)\n",
        "        i = j\n",
        "\n",
        "    return merged_chunks\n",
        "\n",
        "####################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Download the video\n",
        "    video_url = 'https://www.youtube.com/watch?v=zdmEzqpAG70'\n",
        "    download_audio(video_url)\n",
        "    ####\n",
        "\n",
        "    ## Preprocesss the audio\n",
        "    preprocess_audio('audio_extracted.wav')\n",
        "    #####\n",
        "\n",
        "    ## Returns a transcription with word level timestamps\n",
        "    transcription = transcribe_audio_with_timestamps('audio_processed.wav',)   # Transcription: Dict[[str,Dict[str,List[float]]]]\n",
        "    ###\n",
        "\n",
        "    text = transcription[\"text\"]   ### Entire transcribed text as a single string\n",
        "    punctuated_text = punctuate(text)  ## Punctuate the text\n",
        "    transcription[\"text\"] = punctuated_text  ## Update the text with punctuated text\n",
        "\n",
        "    list_of_sentences = split_into_sentences(transcription[\"text\"])    ## Split the punctuated string into a list of sentences\n",
        "\n",
        "    format_data_chunks(transcription[\"chunks\"])   ### Clean the text provided by Whisper (remove all the punctuations)\n",
        "\n",
        "    ### transcription[\"chunks\"] is a list of dictionaries. Each dictionary has keys 'text' (a single word from the transcription) and 'timestamps' (List[start,end])\n",
        "    chunks = create_proper_chunks(list_of_sentences,transcription[\"chunks\"])  # This function takes input the word level timestamps and all the punctuated sentences and returns a list of dictionaries\n",
        "    # chunks is a list of dictionaries. Each dictionary has \"text\" (a single sentence),'start_time' (float) and 'end_time' (float)\n",
        "\n",
        "    ## Collect all the chunks that are bigger than 15s\n",
        "    filtered_chunks = []\n",
        "    for i in range(len(chunks)):\n",
        "        if not check_if_chunk_is_long_enough(chunks[i]):\n",
        "            filtered_chunks.append(chunks[i])\n",
        "\n",
        "    ## Reduce the big chunks into smaller chunks by dividing at a comma\n",
        "    reduced_chunks = reduce_to_less_than_15(filtered_chunks,transcription[\"chunks\"])\n",
        "\n",
        "    ## Check again if all the chunks are less than 15s\n",
        "    for i in range(len(reduced_chunks)):\n",
        "      if check_if_chunk_is_long_enough(reduced_chunks[i]):\n",
        "        chunks.append(reduced_chunks[i])    # Append all the chunks that are less than 15s\n",
        "\n",
        "      else:     # If suppose there exists chunks greater than 15s, break them down again once more.\n",
        "        further_reduced = reduce_to_less_than_15([reduced_chunks[i]],transcription[\"chunks\"])\n",
        "        for j in range(len(further_reduced)):\n",
        "            if check_if_chunk_is_long_enough(further_reduced[j]):\n",
        "                chunks.append(reduced_chunks[i])\n",
        "\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    # Remove all the chunks bigger than 15s. Earlier we have just filtered the bigger chunks but we did not remove them\n",
        "    filtered_chunks = []\n",
        "    for i in range(len(chunks)):\n",
        "        if check_if_chunk_is_long_enough(chunks[i]):\n",
        "            filtered_chunks.append(chunks[i])\n",
        "\n",
        "    processed_chunks = clean_chunks(filtered_chunks)   # Do some cleaning on the text. Join small chunks (with less than 3 words) together while ensuring they don't exceed 15s\n",
        "    # print(processed_chunks)\n",
        "\n",
        "    merged_data = merge_chunks(processed_chunks)    # Merges semantically similar and continuous chunks of text based on embeddings while ensuring they don't exceed 15s.\n",
        "\n",
        "    output = prepare_for_output(merged_data)\n",
        "    print(output)\n"
      ]
    }
  ]
}